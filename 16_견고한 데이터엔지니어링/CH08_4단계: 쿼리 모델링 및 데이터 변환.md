# CH08 4단계: 쿼리 모델링 및 데이터 변환

---

## 쿼리

---

### 쿼리란 무엇인가?

- Read
- DDL
  - CREATE, DROP, ALTER 등 DB, 스키마, 테이블, 사용자와 같은 객체에 대한 작업을 수행함
- DML
  - SELECT, INSERT, UPDATE, DELETE, COPY, MERGE
- DCL
  - 데이터 접근 제한
  - GRANT, DENY, REVOKE
- TCL
  - 트랜잭션의 세부 정보를 제어
  - COMMIT, ROLLBACK


### 쿼리 수명

- 순서
  - SQL을 컴파일해 코드를 파싱 및 바이트 코드로 전환
  - 쿼리 플래닝과 최적화
  - 쿼리 실행
  - 결과 변환

### 쿼리 옵티마이저

- 쿼리를 효율적인 순서로 적절한 단계로 분할해 쿼리 성능을 최적화하고 비용을 최소화
- 조인, 인덱스, 데이터 스캔 크기 및 기타 요소를 평가

### 쿼리 성능 향상

- 조인 전략 및 스키마 최적화
  - 데이터를 선조인하여 반복 작업 하지않도록 함
  - 행 지향 데이터 베이스에서는 인덱스를 통해서 조인하도록 하기
  - 서브쿼리, temp table 대신 **CTE(Common Table Expression)을 사용하여 가독성을** 최대한 늘리자
- 실행 계획을 사용한 쿼리 성능 파악
  - EXPLAIN
    - 디스크, 메모리, 네트워크 등 리소스 사용량
    - 데이터 로딩 시간과 처리 시간
    - 퀴리 실행 시간, 레코드 수, 스캔한 데이터의 크기, 데이터 셔플링에 사용된 데이터양
    - 데이터베이스에서 리소스 경합을 일으킬 수 있는 경쟁 쿼리
    - 동시 연결 수

### 전체 테이블 스캔 방지

- 특정 컬럼만 뽑기
- 가지치기
  - OLAP는 파티셔닝키와 클러스터링키를 지정
  - OLTP는 인덱스를 중심으로
- 데이터베이스가 커밋을 처리하는 방법
  - 트랜잭션을 지원
  - 더티 리드가 예상되는 결과인지?
  - PostgreSQL은 행잠금이 필요하여 대량의 데이터에 최적화되지 않음
  - 빅쿼리
    - 풀 테이블 커밋 모델을 사용하여 최신 커밋된 스냅숏만 읽음
    - 한 번에 하나의 쓰기 작업만 허용 동시성 제공하지 않고 큐잉됨
  - 몽고DB
    - 가변적 일관성 데이터베이스
    - 높은 쓰기 성능이나 데이터 손실이 있음
    - 커밋, 일관성 모델, 기타 기술 성능의 모든 측면에 적용
- vacuum dead record
  - 오래된 레코드는 마지막 상태에 대한 포인터로 유지되고 Vacuuming이라는 프로세스를 통해 제거
  - vacuum
    - 새로운 레코드를 저장할 수 있는 공간을 확보해 테이블 크기를 줄이고 쿼리를 더 빠르게 수행할 수 있음
    - 새롭고 관령성 있는 레코드는 쿼리 계획이 더 정확해짐
    - 좋지 못한 인덱스를 정리함
  - 빅쿼리는 7일 간격의 `time-travel` 이력 윈도를 지원함
  - 데이터 브릭스는 무기한 유지
  - redshift는 백그라운드에서 실행되나 튜닝을 위해 수동으로 실행할 수도 있음
  - RDB는 더욱 중요
- 캐시된 쿼리 결과 활용
  - 자주 실행되는 쿼리의 경우 캐싱처리를 할수 있도록 하기
  - MV도 괜찮

### 스트리밍 데이터에 대한 쿼리

- fast follower
  - 운영 DB에는 대규모 분석 검색을 할 수 없음
  - CDC를 분석 DB에 따로 두는 방식을 사용
  - 이벤트를 동적으로 트리거할 수는 없음
- kappa architecture
  - 이벤트를 스트림으로 저장함
  - 스트림 스토리지에 직접 쿼리할 수 있음 ex) Kafka KSQL
  - 원천과 결합하거니 쿼리가 복잡할 경우 외부도구라 스트리밍에서 읽고 원천에서 읽어서 결합할 수도 있음
  - flink, beam과 같은 스트림 프로서세에 데이터를 공금할 수 있음
- 윈도, 트리거, 출력된 통계, 늦게 도착한 데이터
  - 윈도는 동적 트리거를 기반으로 하는 작은 배치임
    - 세션윈도
      - 사용자 세션을 그룹화
      - 5분간 활동이 없는 상태면 계산하여 데이터 전송
      - 세션을 활용해서 세션이 종료된 후 하루후에 이메일 발송 등 알림을 유용하게 쓸 수 있음
    - 고정 시간 윈도
      - tumbling
      - 기존 배치와 유사하지만 시간이 짧다
    - 슬라이딩 윈도
      - 윈도가 겹치도록 고정된 시간 길이
    - 워터마크
      - 윈도에서 사용하는 임계값
      - 윈도에 새로 도착했지만 워터마크 타임스탬프보다 오래된 데이터는 늦게 도착한 데이터로 간주됨
  - 스트림과 다른 데이터 결합
    - 기존 테이블 조인
    - enrichment: 다른 데이터어와 조인
    - 스트림 간 조인

## 데이터 모델링

---

### 데이터 모델이란?

- 데이터가 어떻게 구조화되고 표준화되는지
- 모델을 비즈니스 성과로 변환하는데 초점을 맞출 것
- 좋은 데이터 모델은 일관된 정의가 포함됨

### 개념적, 논리적, 물리적 데이터 모델

- 개념적 데이터 모델
  - 엔티티 관계를 시각화할 것
  - 비즈니스 논리 및 규칙을 포함하고 스키마, 테이블, 필드와 같은 시스템 데이터 설명
- 논리적 데이터 모델
  - 더 많은 세부정보를 추가하여 개념 모델이 실제로 구현되는 방법을 설명
- 물리적 데이터 모델
  - 데이터에 대한 정의와 비즈니스 목표 확보
  - 인사이트와 지능형 자동화를 위한 고품질 데이터를 기업에 제공하는 것이 목표
  - data grain
    - id
    - timestamp

### 정규화

---

- 정규형 종류
  - 제1정규형: 각 열은 고유하며 단일값을 가지며 테이블에는 고유한 기본 키가 있다.
  - 제2정규형: 제1정규형 + 일부 종속성 제거
  - 제3정규형: 제2정규형 + 각 테이블에는 기본 키와 관련 된 필드만 포함되며 전이 종속성은 없음
    - partial dependency는 복합 키의 필드 하위집합으로 테이블의 키가 아닌 열을 지정할 수 있을 때
    - transitive dependency는 키가 아닌 필드 키가 아닌 다른 필드에 의존할 때 발생
  - 예시
    - orderDetail 테이블 + 기본(복합)키
    - orders 테이블 + orderLineItem 테이블로 분리하여 부분종속성 해결
    - sku - produnctname끼리 의존성이 발생하여 orderLineItems와 Skus로 분리
- OLAP에서는 반정규화 데이터를 많이 활용함

### 배치 분석 데이터 모델링 기술

---

- inmon, kimball, data vault등 데이터 모델링 방식을 사용함
- inmon
  - **source와 DW를 분리할 것**
  - DW 특징
    - 특정 주제(마케팅, 영업)에 초점을 맞춤
    - 통합
    - 비휘발성
    - 시간 변이성
  - Souce에서 ETL => DW에 적재 => DW에서 ETL을 통해 DM 만들기 => BI
- kimball
  - DW자체에서 분석과 모델링을 하는 것을 권장
  - **팩트 테이블**
    - 변경되지 않으며 이벤트관련 데이터
    - 최소의 grain이여야 함
    - 다른 팩트테이블을 참조하지않고 오로지 차원만 참조함
    - ex)
      - orderid, customerkey, datekey, crosssalesamt
      - 고객 및 날짜 정보 같은건 차원 테이블을 참조하는 키를 갖고 있음
  - **차원 테이블**
    - 참조 데이터, 속성 및 관계 컨텍스트를 제공함
    - ex)
      - 날짜 차원: datekey, date-iso, year- quearter, month, day-of-week
      - 고객 차원: customerkey, firstname, lastname, zipcode, eff_Startdate, eff_enddate
    - 차원 변경을 추적하려면 slowly changing dimenstion이 필요함
      - 기존 레코드를 덮어쓰기
      - 레코드를 추가 생성하여 전체 이력을 유지
      - 전체 이력을 유지하되 컬럼 추가하는 방식으로
  - 스타 스키마
    - 필요한 차원으로 둘러싸인 팩트 테이블
    - 조인수가 적어 쿼리성능 향상되고 사용자가 쉽게 이해하고 사용할 수 있음
    - 비즈니스 로직의 팩트와 속성을 포착하고 질문에 답변할 수 있또록 유연해야 함
    - `conformed dimension`을 구성하여 여러 스타 스키마에서 재사용하도록 하여 차원 수를 줄여야 함
  - 배치 데이터에 적합
- DataVault
  - 구조적 측면을 분리함
  - 데이터 모델은 민첩하고 유연하며 확장 가능해야 함
  - 비즈니스에 최대한 밀접하게 연계
  - 3가지 모델을 결합해여 사용
    - 허브
      - 비즈니스 키를 저장
      - 고객 id, 주문 id등
      - 입력 전용이며 변경 되지 않음
      - ex) Hashkey, loaddate, recordSource, productID
    - 링크
      - 비즈니스 키 간의 관계를 유지
      - 허브 간 비즈니스 키의 관계를 추적
      - ex) hashKey, loadDate, recordSource, PruductHashkey, OrderHashKey(허브)
    - 위성
      - 비즈니스 키의 속성과 컨텍스트를 표현
      - ex) hashKey, loadDate, recordSource, productName, Price
- 넓은 비정규화 테이블
  - 스토리지 비용이 매우 저렴함
  - 중첩데이터가 많이 생김
  - RDB와 달리 컬럼형 데이터베이스는 NULL에 대한 공간과 읽기 비용이 발생하지 않음
  - 많은 조인보다 빨리짐
  - 데이터 모델링에 관심이 없거나 유연성이 필요하다면 와이드 테이블이 더 좋음

### 스트리밍 데이터 모델링

---

- 데이터 모델 방식은 아직 없음
- 소스 데이터 변화를 예측하고 유연한 스키마를 유지할 것을 제안
- 스트리밍 데이터의 이상이나 변경에 반응할 수 있는 자동화를 구축

## 변환

---

- 데이터를 조작, 강화 및 저장해 다운스트림에 사용할 수 있도록 함으로써 확장 가능하고 안정적이며 비용 효율적인 방식으로 데이터의 가치를 높임
- 쿼리와 달리 복잡하고 결과를 계속 사용할 수 있도록 유지한다.

### 배치 변환

---

- 분산 조인
  - broadcast join
    - ![broadcast join](https://miro.medium.com/v2/resize:fit:1400/1*pO_40cT0UhaiSP0fdT-sWw.jpeg)
    - 하나의 큰 테이블이 여러 노드에 분산되어 있음
    - 작은 테이블이 큰테이블 노드에 조인함
    - shuffle hash join보다 리소스가 더 적게 듬
    - join reordering은 필터를 조기에 적용하고 작은 테이블을 왼쪽으로 이동하면 각 조인에서 처리되는 데이터양을 크게 줄일 수 있음
  - shuffle hash join
    - ![shuffle hash join](https://miro.medium.com/v2/resize:fit:1400/1*Yjw7V8mh7FipB09ngnBn6A.jpeg)
    - 모든 테이블이 단일 노드에 들어갈 수 없을 때 사용
    - 초기 파티셔닝 후에 조인키 기준으로 다시 파티셔닝하여 사용
- ETL, ELT, 데이터 파이프라인
  - 전통적인 방식은 데이터 가져오고 변환하고 클렌징하고 로드
  - ELT의 기존방법은 DW에서 처리
  - 최근 방법은 데이터 레이크로서 객체 스토리지를 기본 계층으로 활용
  - 이제는 표준화하기보다는 적절한 기술을 사례별로 적용하는데 집중해야 함
- SQL과 코드 기반 변환 도구
  - 선언적이고 복잡한 로직이나 테스트 코드 등을 적용할 수는 없지만 여전히 강력한 도구
  - 단어 접미사 추출이나 어간 변환 등 복잡한 로직은 프로그래밍 언어를 사용하는 것이 좋음
  - 라이브러리나 재사용코드로 관리하기엔 프로그래밍 언어가 유용
  - Spark Native활용시 작업 방식을 이해하고 최적화를 해나가야 함
- 갱신 패턴
  - 데이터 삭제와 재적재
    - truncate and reload
  - 입력 전용
    - insert only
    - 최신 레코드를 찾는 데 연산 비용이 많이 들 수 있음
  - 삭제
    - 데이터 레이크에서 삭제는 입력보다 더 큰 비용이 든다
    - 물리삭제, 논리삭제(삭제 표시)
    - 삭제 레코드 입력 - 최신 상태를 확인할려면 조금 더 복잡해질 수 있음
- 갱신 입력과 병합
  - upsert + merge
  - Copy On Write이기 때문에 레코드를 변경 및 삭제하는 것이 쉽지 않음
  - 적절한 파티셔닝과 클러스터링 전략, 빈도를 고려하여 실시간에 가까운 갱신입력을 하기
- 스키마 갱신
  - 반정형 데이터는 자주 사용하는 필드만 flattened field로 저장
- 데이터 랭글링
  - data wrangling
  - 잘못된 형식의 데이터를 깨긋한 데이터로 변환하는 작업
  - 추론된 유형, 분포를 포함한 통계, 이상 데이터, NULL 등을 표시해줌
  - 특잇값을 다루기 위해 레시피를 더욱 정교하게 만들 수 있음
  - 예시
    - 스파크
      - s3에 있는 3개의 데이터를 스파크 잡을 에어플로로 트리거하여 데이터 프레임으로 수집
      - JSON을 관계형식으로 변환 후 결합하여 SQL문으로 결과 필터링
      - Parquet형식의 Delta Lake Table에 기록됨
- 비즈니스 로직과 파생 데이터
  - 단순히 수익을 계산할때만 해도 측정 기준과 복잡성이 있음
  - 비즈니스 로직이 변경될 경우 ETL 스크립트를 변경 및 재수행 하는것은 매우 번거로움
  - 비즈니스 로직을 Metric Layer에 적용하여 부하가 높은 작업을 다른 도구로 수행하도록하는 것
- 맵리듀스
  - 분산 시스템의 기본
  - map -> shuffle -> aggregation
    - Map은 노드 수에 따라 선형적으로 확장되어 여러 블록을 한꺼번에 읽어 올 수 있음
    - reduce는 키별로 결과를 재분배해 키에 대한 결과를 합산함
- 맵리듀스 이후
  - 중간 상태는 보존하지 않고 메모리 사용률이나 네트워크 통신이 커짐
  - 요즘엔 대부분 메모리와 SSD를 사용하여 빠르게 처리하는 것이 이익으로 판단

### 구체화된 뷰, 페더레이션, 쿼리 가상화

---

- 뷰
  - 테이블을 참조하는 쿼리
  - 보안역할 - 모든 데이터를 다 보여줄 필요가 없음
  - 중복이 제거된 데이터 현재 모습
  - 공통 데이터 접근 패턴 - 여러개 테이블 조인
- 구체화된 뷰
  - 뷰의 일부 또는 전부를 미리 연산함
- 구성 가능한 구체화된 뷰
  - databricks live table이라는 개념처럼 각 테이블은 소스로부터 데이터가 도착하면 갱신되며, 데이터는 비동기적으로 후속테이블로 흘러감
- Federated Query
  - OLAP에서도 object storage나 rdbms에 조회할 수 있도록 하는 기능
- Data Virtualization
  - 데이터를 내부에 저장하지 않는 데이터 처리 및 쿼리 시스템을 수반함
  - Trino, Presto
  - 중요한 요소는 외부 소스와 성능임
  - Query Pushdown
    - 소스쪽으로 많은 작업을 이동시키는 것이 목표
      - 가상화 계층의 연산 부하 줄이기
      - 데이터양 감소
  - 트리노로 MySQL을 조회하여 S3에 저장할 수 있음
  - 데이터 레이크를 더 많은 소스로 확장하는 도구

### 스트리밍 변환과 처리

---

- 기본
  - 스트리밍 저장소와 스트림 프로세서를 결합함으로써 오랫동안 가능함
- 마이크로 배치 vs 스트리밍
  - 진정한 스트리밍 시스템은 한번에 하나의 이벤트를 처리하도록 설계 됨
  - 둘중에 정답은 없음

### 함께 일할 담당자

---

- 업스트림
  - 원천 시스템의 데이터 변경이 다운스트림에 직접적인 영향을 미칠 수 있음
- 다운스트림
  - 비즈니스는 변환된 데이터가 정확하고 실행 가능한 데이터

### 드러나지 않는 요소

---

- 보안
  - 읽기/쓰기 권한
  - 자격 증명
  - 암호화된 데이터
- 데이터 관리
  - 명명 규칙
  - 데이터 카탈로그 관리
  - 정의적 정확성
  - 데이터 계보
  - 데이터 마스킹

### 데이터 옵스

---

- 데이터와 시스템이라는 두 가지 관심
  - 데이터
    - 입출력 올바른지
    - 스키마 올바른지
    - 기본통계
    - 품질 테스트
    - 롤백
  - 옵스
    - 시스템의 성능
    - 쿼리 대기열 길이
    - 쿼리 동시성
    - 메모리 사용량
    - 스토리지 사용률
    - 지연률
    - 병목 현상

### 데이터 아키텍처

---

- 데이터를 파괴하지 않고 처리 및 변환할 수 있는 강력한 시스템

### 오케스트레이션

---

- 복잡한 파이프라인을 위한 오케스트레이션 사용

### 소프트웨어 엔지니어링

---

- 다양한 언어와 도구의 모범사례를 알아야 함
- 데이터 도구의 진입 장벽을 낮게하고 민주화를 이룸으로써 워크플로를 발전시켜나가기

## 결론

---

- 데이터 파이프라인의 중심은 변환