# CH07 3단계: 데이터 수집

---

## 데이터 수집이란?

---

- 데이터를 한 장소에서 다른 장소로 옮기는 프로세스

## 수집 단계의 주요 엔지니어링 고려 사항

---

- 유한데이터 vs 무한데이터
  - All data is unbounded until it`s bounded
- 빈도
  - 실시간이 점점 빈번해짐
  - 스트리밍 처리는 일반적으로 배치 처리와 공존함
- 동기 수집 vs 비동기 수집
  - 동기는 개별 배치 단계로 실행되며 장애시 재수행하면 오래 걸림
  - 비동기로 할 시 데이터 스트림이 충격 흡수 장치가 됨
- 직렬화와 역직렬화
  - 직렬화란 원천으로부터의 데이터를 인코딩하고 데이터 구조를 준비하는 것을 의미
  - 타겟쪽으로 역직렬화할 수 있는지 확인
- 처리량과 확장성
  - 처리량에 따라 유연하게 확장 및 축소 가능한지 살펴보기
  - 처리량 확장을 처리하는 관리형 서비스를 사용
- 신뢰성과 내구성
  - 신뢰성은 수집 시스템의 Availability & Failover
  - 내구성은 데이터가 손실되거나 손상되지 않도록 하는 것을 의미
  - 데이터 손실에 따른 영향과 비용을 바탕으로 리스크 평가하고 적절한 수준의 중복성과 자가 복구 기능을 구축하는 것이 좋음
- 페이로드
  - 데이터셋의 종류, 형태, 크기, 스키마, 데이터 유형, 메타데이터 등의 특성
  - 종류
    - 유형(표, 이미지, 비디오)과 형식(확장자, 이름, 바이트패턴)으로 구분
  - 스키마
    - 변경
      - 열 추가
      - 유형 변경
      - 새로운 테이블
      - 열  이름 변경
    - **변경에 자동으로 대응하고 할 수 없는 경우 알람을 보내는 전략**
    - 스키마 레지스트리
      - 스키마 및 데이터 유형의 무결성을 유지하는 데 사용되는 메타데이터 저장소
    - 메타데이터
      - 데이터에 대한 설명
- 푸시 vs 풀 vs 폴링 패턴
  - source -> puse -> target
  - pull은 데이터를 직접 읽기
  - polling은 데이터원천에 변경 사항이 없는지 정기적으로 확인. 풀링과 유사

## 배치 수집 고려 사항

---

- time interval batch ingestion
  - daily한 수집
- size based batch ingestion
  - 이벤트에 대한 크기나 객체스토리지의 블록 단위로 수집
- 일반적인 배치 수집 패턴
  - 스냅숏 또는 차등 추출
    - differential update(incremental update) 네트워크 트래픽과 타깃 스토리지 최소화
    - full snapshot 단순함
  - 파일 기반 익스포트 및 수집
    - 직접적인 데이터베이스 연결 접근 방식에 비해 몇 가지 잠재적인 장점이 있음
      - 보안상
      - 운천엔지니어가 데이터 전처리 방법을 완벽하게 제어할 수 있음
- ETL과 ELT
- 입력, 갱신 및 배치 크기
  - 컬럼형 DB에 OLTP적인 쿼리를 날리면 성능이 좋지 않음
  - 기존 열 파일을 검사해야 하므로 더 큰 문제가 발생함
  - 갱신 패턴도 높은 삽입속도를 처리하는 pinot, druid가 있음
- 데이터 마이그레이션
  - 스키마 관리


## 메시지 및 스트림 수집에 관한 고려사항

---

- 스키마 진화
  - 스키마 레지스트리로 버전화하기
  - DLQ로 실패한 이벤트 삽입
  - 잠재적인 스키마 변경에 관해 업스트림 이해관계좌와 정기적으로 소통하기
- 늦게 도착하는 데이터
  - 동일한 시간에 이벤트가 발생했으나 일부만 늦게 수집 될 수도 있음
- 주문 및 복수 전달(at least once)
- 재생
- 유효 시간(TTL, Maximum message retntion time)
- 메시지 크기
- 에러 처리와 데드레터 큐
- 소비자 풀 앤 푸시: 되도록 푸시로 고려할 것
- 위치: 데이터가 얼마나 가까운지

## 데이터 수집 방법

---

- 직접 데이터 베이스 연결 - JDBC/ODBC
  - JDBC는 자바 드라이버지만 JVM의 하드웨어 아키텍처 및 운영 체제 간에 이식 가능한표준이면 Just In Time 컴파일러를 통해 컴파일된 코드의 성능을 제공
  - ODBC는 OS 및 아키텍처 네이티브 바이너리로 제공
  - RDB 수집에 널리 사용됐으나 최근에는 Export기능을 많이 활용
- CDC
  - 배치 CDC
    - updated_dt 같은 필드로 배치식으로 구현할 수 있으나 주기에 따라 데이터 변경 유실이 발생함
  - 연속 CDC
    - log-based CDC
    - 데베지움 같은 스트리밍 플랫폼으로 내보냄
  - CDC와 데이터베이스 복제
    - read replica를 구성하여 분석용 DB를 구분
  - CDC 고려사항
    - 운영 팀과 협력해 테스트를 구성할 것
- API
  - 표준이 없어 어려움
    - API 클라이언트 라이브러리 제공
    - 다양한 데이터 커넥터 플랫폼 등장
    - 데이터 공유
  - API 커넥터로 할 때 기회비용을 잘 파악할 것
- 메시지 큐와 이벤트 스트리밍 플랫폼
  - 메시지는 소비되면 큐에서 삭제됨
  - 스트림은 순서가 있는 로그로 수집됨
  - 실시간 수집 파이프라인에 대한 운영도 필요
- 관리형 데이터 커넥터
  - 데이터 수집이 필요할 경우 커넥터가 있는지 우선 파악할 것
- 객체 스토리지로 데이터 이동
  - 확장성고 ㅏ신뢰성에 대한 강력한 실적
- EDI
  - 전자 문서 교환
  - 데이터 다운로드와 동시에 메일 보내기 등
- 데이터베이스와 파일 익스포트
  - 소스에 영향 없이 익스포트하여 활용
- 공통 파일 형식의 실질적문제
  - parquet, orc는 열을 직접 변환할 수 있어 컬럼형 DB에서 export할 때 효율적
  - CSV는 스키마가 아님
- Shell
- SSH
- SFTP & SCP
- Webhook
- Web Interface
- Web Scraping
  - IP 차단
  - 법적 의미
  - HTML 변경이 잦으면 유지보수가 큼
- 데이터 마이그레이션 컴플라이언스
- 데이터 공유

## 함께 일할 담당자

---

- 업스트림 이해 관계자
  - 커뮤니케이션
  - 엔지니어가 기여한 공헌을 강조해야 함
  - 그래야 이벤트 기반 아키텍처 등 다양한 프로젝트가 가능함
- 다운스트림 관계자
  - **소통**
  - 자동화하여 데이터기반문화에 근접하도록 하기ㅏ

## 드러나지 않는 요소

---

- 보안
  - VPC 경계
  - VPN or private link
  - encryption
- 데이터 관리
  - 스키마 변경 - git으로 관리
  - 데이터 윤리, 개인정보보보호, 컴플라이언스
    - 암호화 키에대한 엄격한 관리
- 데이터 옵스
  - 수집단계에 대한 모니터링
    - 이벤트 생성, 수집, 프로세스 및 처리 시간 등
    - 이벤트 평균 크기를 알아야 함
    - 서드 파티의 장애도 생각해야 함
  - 데이터 품질
    - 데이터 변경을 캡처하고 예외 처리하는 것만으로도 데이터 품질 문제를 처리할 수 있음
    - null 감지, 예기치 않은 품목 등
- 오케스트레이션
- 소프트웨어 엔지니어링
  - 데이터 커넥터 사용
  - 코드 분리 등
